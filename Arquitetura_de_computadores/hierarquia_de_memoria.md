# Notes about Hierarquia de Mem√≥ria :)
##### üìåTerceira Unidade da disciplina "Arquitetura de Computadores" do curso de BTI na UFRN


Sum√°rio
1. [Defini√ß√£o de Mem√≥ria](#m√©moria-√©-um-lugar-para-o-armazenamento-de-informa√ß√µes-e-suas-√∫nicas-a√ß√µes-s√£o-leitura-e-escrita)
2. [Memory Wall](#memory-wall)
3. [Hierarquia de Mem√≥ria](#hierarquia-de-mem√≥ria)
4. [CACHE](#cache)
5. [CACHE - Decis√µes de Projeto](#decis√µes-de-projeto)


#### __M√©moria__  √© um lugar para o armazenamento de informa√ß√µes, e suas √∫nicas a√ß√µes s√£o: leitura e escrita.

- Mem√≥ria endere√ß√°vel - Pois via um endere√ßo, voc√™ pode ter acesso a um determinado dado.

Qual √© o tamanho ideal? O maior tamanho poss√≠vel e a localiza√ß√£o mais perto do processador.

O que temos? Tecnologias de mem√≥ria velozes, mas s√£o muito caros. Por outro lado, temos algumas baratas, mas s√£o muito lentas.

#### Memory Wall
- Realizar o processamento √© muito mais r√°pido hoje em dia, do que busca o/a dado/instru√ß√£o na mem√≥ria. No gr√°fico temos o "PERFORMANCE GAP", da evolu√ß√£o da mem√≥ria e do 
processador ao passar dos anos.
- Quando eu tenho os dados se torna mais f√°cil fazer a conta!!
- Como posso melhorar isso??

# Hierarquia de Mem√≥ria
- Registradores: Eles s√£o muito caros de se fabricar üò´ Mas s√£o mem√≥rias cada vez __mais r√°pidas__.
- Mem√≥ria Cache: Cache L1,L2,L3...s√£o os n√≠veis, quanto menor o n√∫mero mais pr√≥ximo ao processador ela est√°
  - Os registradores e a cache L1 est√£o DENTRO DO PROCESSADOR, dependendo do processador, ele tamb√©m pode ter a cache L2 e L3 dentro do processador
- Mem√≥ria Principal: Mem√≥ria RAM (Random Acess Memory), o custo por BIT √© mais barato do que o da mem√≥ria cache, mas ela fica FORA do processador.
- Mem√≥ria Externa: (Ou mem√≥ria secund√°ria, mem√≥ria de massa) HD¬¥s, mem√≥ria Flash. O custo por BIT √© menor do que todos os custos por BIT de mem√≥ria j√° citados anteriormente.

Mais acima da pir√¢mida: Maior o CUSTO (o pre√ßo baseado em custo por BIT, s√£o mais caras) e maior a VELOCIDADE (a rapidez de resposta de busca na mem√≥ria).
Mais abaixo na pir√¢mide: Maior o TEMPO DE ACESSO (ou seja, mais lento) e Maior a CAPACIDADE (pode armazenar mais)

 
 #### Como s√£o formadas?
 
 - Mem√≥ria Cache: Tecnologia √© a SRAM (Static Random Acess Memory).
 - Mem√≥ria Principal: Tecnologia √© a DRAM (Dynamic Random Acess Memory)
 - Mem√≥ria virtual (ou externa): Tecnologia √© HDD, SSD.

<div align="center"> 
  <image src="images/disco_computador.png" width=350 height=200>
</div>

<div align="center">
  <image src="images/disco_celular.png" width=350 height=200>
</div>


- __Hierarquia Inclusiva__: Tudo o que est√° dentro do registrador est√° dentro da cache, tudo o que est√° dentro da cache, est√° na principal, e tudo o que est√° na principal est√° no disco. Ou seja, s√£o sub-conjuntos uma da outra, baseado na ordem de hierarquia.

   - Os discos r√≠gidos e as mem√≥rias principais s√£o os √∫nicos N√ÉO VOL√ÅTEIS. Assim, para acessar, os dados v√£o sendo copiados at√© chegar aos registrados e aos usu√°rios.

  - A mem√≥ria principal pode ser acessada pelo processador.
  - A mem√≥ria secund√°ria n√£o pode ser acessada diretamente! O dado DEVE PASSAR PELA MEM√ìRIA PRINCIPAL para poder ser lido/escrito na mem√≥ria secund√°ria.

<div align="center">
 
 # __CACHE__
 </div>

 Para melhorar o tempo de busca do dado dentro da mem√≥ria, a cache foi criada. Assim, quando o processador busca alguma informa√ß√£o na mem√≥ria, inicialmente ela:
  - Verifica se existe dentro da mem√≥ria cache. Caso sim, h√° um _cache hit_. Caso n√£o esteja l√°, √© um _cache miss_.

A ordem de comunica√ß√£o ocorre nesta ordem:

CPU -> Cache L1 -> Cache L2 -> Cache L3 -> Mem√≥ria Principal -> DISCO

- Se o dado estiver na Mem√≥ria principal, o dado √© devolvido para a mem√≥ria cache, e de l√°, devolve-se para a CPU por meio da repeti√ß√£o da busca da CPU na mem√≥ria Cache.

- Da CPU a Mem√≥ria Principal, tudo √© feito pelo Hardware (exist√™ncia do dado, transfer√™ncia do dado).
- J√° a partir do DISCO, e executado pelo Sistema Operacional (SO).

A CACHE guarda sempre as √∫ltimas informa√ß√µes solicitadas pelo computador.

> üìö __PRINC√çPIO DA LOCALIDADE__ : √â uma _guess_ de comportamento de usu√°rio, e de pedidos do processador. Diz que, se um item (leitura/escrita) foi referenciado, h√° uma probabilidade de que ele seja acessado novamente (LOCALIDADE TEMPORAL) ou que um de seus vizinhos sejam acessados daqui h√° pouco (LOCALIDADE ESPACIAL).

- Exemplo de Localidade Temporal: LOOPS
- Exemplo de Localidade Espacial: ESTRUTURAS - VETORES, MATRIZES, STRUCTS...

Esses princ√≠pios s√£o seguidos por serem nossa maneira de programar h√° muito tempo. E essa localidade faz com que a hierarquia de mem√≥ria (guardando os sub-conjuntos) funcione de forma usual. -----

Implementei um exemplo de acesso por coluna/linha de uma matriz 1000x1000 e comparei os tempos para conferirmos como custa mais para a mem√≥ria procurar, baseado neste princ√≠pio da localidade.
Esse √© um exemplo de baixa localidade temporal e baixa localidade espacial. Pois a mem√≥ria cache n√£o guarda os dados na mesma forma em que foram guardados na mem√≥ria

> [Exemplo de tempo de execu√ß√£o do programa com mem√≥ria cache e uma matriz 1000x1000](main.cpp)

```c++ 
auto inicio =  std::chrono::steady_clock::now();

    for(int j = 0; j < 1000; j++){
        for(int i = 0; i < 1000; i++){
            soma += my_matrix.at(i).at(j);
        }
    }
    auto fim = std::chrono::steady_clock::now();
    std::chrono::duration<double> diff_2 = fim - inicio;
```

Perceba que neste trecho do c√≥digo, o iterador percorre o vetor de uma forma n√£o convencional, e assim, n√£o da forma em que est√° guardada na mem√≥ria. Como o princ√≠pio da localidade considera essa forma de programa√ß√£o, ele sofre um pouco ao tentar ser aplicado neste caso, j√° que os 'vizinhos' s√£o diferentes.


- √â uma mem√≥ria r√°pida, de pequena capacidade. Itens referenciados com frequ√™ncia.

- Entre CPU e Cache, apenas o dado requerido √© devolvido. Mas entre Cache e Mem√≥ria Principal, por conta do PRINC√çPIO DA LOCALIDADE, h√° uma transfer√™ncia de blocos, a cache tamb√©m recebe os vizinhos dos dados.
<div align="center">

  #### T E R M I N O L O G I A S

</div>

  - Taxa de _hit_: Quantas vezes ocorreu um _cache hit_, em que o dado estava na cache.
  - Taxa de _miss_: Quantas vezes ocorreu um _cache miss_, em que o dado n√£o estava na cache.
  - Tempo de hit: Tempo para definir se o dado est√° ou n√£o na cache, e entregar o dado para a mem√≥ria principal.
  - Penalidade de miss: Tempo para determinar se houve miss, para entregar para a mem√≥ria principal e para que a mem√≥ria principal retorne o dado para a mem√≥ria cache. 
  - Palavra: unidade de sistema de mem√≥ria.
  - Bloco: Coletivo de palavras dentro da mem√≥ria.
  - Linha: Coletivo de palavras dentro da cache. (O bloco, por√©m dentro da cache)
  - Tag: o nome dos blocos, identificadores.

  A mem√≥ria cache atua em solicita√ß√µes entre o processador e a mem√≥ria principal.
    - O resultado dessa solicita√ß√£o √© um _cache hit_ ou um _cache miss_. O que implicar√£o em tempo de miss, ou penalidade de hit.

  ## __DECIS√ïES DE PROJETO__
  1. Qual o tamanho da unidade m√≠nima de transfer√™ncia de dados na mem√≥ria? 
  <br>_Est√° associado ao tamanho da palavra pelo processador. Perceba que se for de 64bits vai precisar de mais mem√≥ria e mais energia_
      - Ap√≥s o _hit_, a cache devolve um endere√ßo, e cada endere√ßo corresponde a uma palavra. Os poss√≠veis endere√ßos a serem referenciados est√£o no __espa√ßo de endere√ßamento__, neste caso (32 bits), de 0 a 1023.

2. Agora, quantas palavras eu trago da mem√≥ria principal (com o vizinhos), qual o tamanho do bloco?
    - O tamanho do bloco √© o mesmo do que vem na mem√≥ria principal e na mem√≥ria cache. 
    - Essa decis√£o de projeto afeta diretamente a quantidade de blocos que existir√£o na mem√≥ria cache.
    - N√£o pode ser muito grande, pois excederia a quest√£o da localidade espacial. Seria fora do escopo, no caso, seria um vizinho muuuuuuito distante.
    - Exemplo: Tamanho da palavra - 32 bits. Cada bloco tem 8 palavras, como temos 1024 palavras..temos 128 blocos!

- Mem√≥ria cache √© bem menor do que a mem√≥ria principal, mas __o tamanho dos blocos √© IGUAL__.

Para descobrir qual bloco o endere√ßo pertence:
```c++
  int B; //bloco
  int E; //endere√ßo do que eu quero
  int N; //tamanho da palavra do processador/bloco
  B = E div N
```

- Tudo vai sendo COPIADO para a mem√≥ria cache.

<div align="center">

__QUEST√ïES DE HIERARQUIA DE MEM√ìRIA__
</div>

1. _POSICIONAMENTO DO BLOCO_

  - Mapeamento direto (directed-mapped)<br>
    Cada bloco est√° em uma posi√ß√£o determinada na cache. Isso facilita MUITO a busca, pois o cache hit/miss √© determinado pela verifica√ß√£o de uma localiza√ß√£o j√° determinada. Desvantagem: H√° um desperd√≠cio de espa√ßo, pode ser que todas as solicita√ß√µes precisem ocupar o MESMO espa√ßo!
    - REGRA DE MAPEAMENTO:
    ```c++
       int pos_na_cache =   end_do_bloco % linhas_da_cache;
    ```
  - Totalmente associativa (fully associative)<br>
    Neste caso, o bloco pode ser colocado em _qualquer posi√ß√£o_. N√£o h√° desperd√≠cio de espa√ßo :) Masss, a busca no pior caso √© ENORME. Por isso, esse tipo de mapeamento s√≥ √© usado em mem√≥rias muito pequenas, ou realiza buscas em paralelo.
  - Associativa por conjunto (n-way set-associative)<br>
    √â um meio do caminho entre os dois anteriores, que s√£o bem extremos. Separa a cache por conjuntos.
    _"way"_ (ou vias), 
    <br> Qu√£o mais alto o n√∫meros de vias, maior o desempenho da cache, mas o estudos mostram que o maior √© de 16 bits.

    C√°lculo:
    ```c++
      int map_conjunto = end_bloco % quant_conjuntos;
    ```
    Uma vez escolhido o conjunto, a√≠ √© s√≥ aplicar o TA, escolhe o que estiver dispon√≠vel/busca em todos desse conjunto. <br> Este caso, √© como se pegasse o melhor dos dois mundos (MD e TA)
2. _SUBSTITUI√á√ÉO DE BLOCO_
    Se o dado n√£o estiver na cache, o processador congela o seu funcionamento (_stall_), e busca na mem√≥ria principal o dado. <br>
    - Algoritmo aleat√≥rio: O bloco substitu√≠do √© um bloco aleat√≥rio.
      - Gerador de n√∫meros aleat√≥rios
    - Menor Recentemente Usado (LRU): substitui-se o bloco que n√£o √© usado h√° mais tempo. Baseado no princ√≠pio da localidade temporal. Neste caso, tenho que ter um registrador que guarde h√° quanto tempo o dado foi solicitado pela ultima vez.
      - Refer√™ncia temporal para cada um;
      - Alto custo de implementa√ß√£o, pois cada bloco precisa de um registrador.
      - Precisa ser sempre redimencionado para evitar colocar blocos que nunca ser√£o usados.
    - Menos Frequentemente Usado (LFU) : substitui-se o bloco que foi menos utilizado. Neste caso, preciso de um contador que me diga que bloco √© mais frequentemente acessado.
      - Registrador que conta quantas vezes foi utilizado
      - √â redimensionado devido √† satura√ß√£o do contador. (Zerado)
    - Primeiro a entrar, primeiro a sair (FIFO): substitui-se o mais antigo, mesmo que tenha sido usado recentemente, o que entrou primeiro na cache.
      - Uma fila para falar "esse √© o mais antigo aqui" <br> <br>
    > O algoritmo __√≥timo__ para substituir na cache, o √ìTIMO, √© um que n√£o conseguimos implementar: "Retire da cache o dado meno prov√°vel de ser utilizado, o mais longe do dado".

3. _ESTRAT√âGIA DE GRAVA√á√ÉO_
 <br> Dois modos de escrita:<br>
    - _Write-through_: A informa√ß√£o √© escrita na cache e na mem√≥ria principal ao mesmo tempo.
      - Dispositivos com m'ais de um processador, onde ambos compartilham uma √∫nica mem√≥ria. [Pode ocorrer um problema, de disposi√ß√£o de corrida (disciplina de SO)]
    - _Write-back_: Escrita pregui√ßosa...S√≥ escreve 
    de volta na mem√≥ria quando a cache precisar se livrar dele, quando ele for substitu√≠do. Economizo em escritas (menos, pois s√≥ vai para a mem√≥ria principal no final de tudo). Problema: H√° uma inconsist√™ncia entre as duas mem√≥rias.
      - Servidores, sistemas embarcados
      - Consome menos largura de banda
